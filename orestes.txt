Orestes: a searchable, scalable timeseries database backed by Elasticsearch and Cassandra

Metrics

Metrics are one of the most useful tools in a devops agent's arsenal. Metric data is defined as repeated measurements of the same quantity as it varies over time. For instance, if I measure the CPU usage on my server every ten seconds while my application is running, that's metric data. Metric data is widely used to analyze the performance of software projects: there are tools like statsd and collectd that make it easy to collect. In a large system with a lot of metrics, it becomes critical to store metric data in a compact way that enables fast, easy queries. The most popular data model for achieving this is the timeseries data model. The timeseries data model groups together measurements of the same quantity. It figures out what metrics are being measured, and it associates each metric with a list of all the measurements of that metric. For instance, let's say I measured the CPU usage on computers A and B at 12:00, 1:00 and 2:00, and I got 10%, 20%, and 30% on computer A; 15%, 25%, and 35% on computer B. The timeseries representation of this data looks conceptually like:
        {metric_name: 'cpu_usage', computer: 'A'} ----> [{time: 12:00, value: 10%}, {time: 1:00, value: 20%}, {time: 2:00, value: 30%}]
        {metric_name: 'cpu_usage', computer: 'B'} ----> [{time: 12:00, value: 15%}, {time: 1:00, value: 25%}, {time: 2:00, value: 35%}]
In a traditional relational data model, you'd store the metric name and computer with each time-value pair. Duplicating information like that takes up a lot of unnecessary space, so a database using the timeseries model stores metric data much more efficiently.

The database that Jut uses for timeseries data is named Orestes. Orestes combines the speed and scalability of Apache Cassandra with the searchability of Elasticsearch to execute flexible, high-performance queries on billions of data points across millions of distinct metrics every day.

Orestes uses Apache Cassandra for most of its data storage and retrieval. This is because Cassandra's data model is easily adapted for timeseries data. In Cassandra, data is divided into "rows". Rows are indexed by a "row key". Each row contains several "columns". A single stored object is represented by the combination of the key-value pairs of a column with the key-value pairs of the row containing it. In a timeseries database backed by Cassandra, each metric being measured is represented by a unique set of key-value pairs. This set of key-value pairs is used as a row key. The columns corresponding to a row key are the times and values of measurement for the metric represented by that row key. So to represent the data above, one of our row keys would be {metric_name: 'cpu_usage', computer: 'A'}, corresponding to the columns [{time: 12:00, value: 10%}, {time: 1:00, value: 20%}, {time: 2:00, value: 30%}]. 

For storing a general JSON object representing a metric point, the row key consists of all the fields of the point other than time and value. Unfortunately, Cassandra requires users to fully specify the fields that make up the rows and columns of a table when that table is created. Therefore, unless you know in advance what all the attributes of all your data will be, there's no built-in way to tell Cassandra to use all the fields besides time and value as a row key. To get around this limitation, Orestes uses a single string field as the row key. The value of this field for a given data point is a serialized string representation of the row-key fields of that point. For instance, Orestes would represent our row key {metric_name: 'cpu_usage', computer: 'A'} as "computer=A,metric_name=cpu_usage": an alphabetized, comma-delimited list of "key=value" susbtrings. This enables very efficient storage of arbitrary timeseries data in Cassandra. Querying a row is very quick, too: given a row key, Cassandra can execute a read on that row in around ten microseconds on a modern laptop.

An Orestes Table

Putting it all together, a sample table-creation statement used by Orestes is "CREATE TABLE IF NOT EXISTS "orestes_default".metrics16688 (attrs varchar,offset int,value double, PRIMARY KEY(attrs, offset)) WITH COMPACT STORAGE". We use CREATE TABLE IF NOT EXISTS to simplify Orestes' table-creation logic so it doesn't have to be concerned with whether a table exists before creating it. The table name encodes a fair bit of information: to the left of the dot is the name of the Cassandra keyspace. Orestes keeps a keyspace for each Jut space that stores metric data, prefixing its name with "orestes_" to avoid collisions with Cassandra's built-in keyspaces. We enclose the name of the keyspace in quotes to allow spaces with special characters. 

On the right side of the dot, we have the name of a particular table. It is just the word "metrics" with an integer suffix. The suffix represents the number of days since January 1, 1970 at the time of that table's creation. The suffix will always be a multiple of 7, because Orestes creates one table per week per keyspace. We went with this model to enable quick deletion of unwanted metric data. To delete unwanted data, Orestes just drops the table containing that data. This means that if you want to delete any data from Orestes, you need to delete the whole week that contains that data. This is an unfortunate restriction, but it was the best way we found of working with Cassandra's deletion quirkiness. Cassandra ostensibly has a delete-by-query API, but this API is a lie. When you delete data using Cassandra's delete-by-query API, it really just places "tombstones" on the datapoints you deleted, to be cleaned up the next time Cassandra performs a compaction on the table with tombstones. Compaction is part of Cassandra's self-regulated maintenance, but it occurs at unpredictable intervals and very infrequently if a table isn't being written to. Tombstoned points still take up as much disk space as non-tombstoned points, so keeping them around for an arbitrary period of time after you've allegedly deleted them was not an option for us. Luckily, Cassandra deletes data and frees disk space immediately upon dropping a table, provided you have "auto_snapshot: false" set in your cassandra.yaml configuration file (with auto_snapshot: true, the default, Cassandra will back up all the data you just tried to delete, and space still won't be freed). So that's how we got Cassandra to actually delete data. We chose a granularity of one week instead of something smaller because Cassandra has scaling problems with many tables: each table requires around a megabyte of heap overhead for pre-allocated buffers and such, so if you have thousands of tables you'll run into memory pressure. With only one table per week, we can safely support scores of spaces.

Moving on to the table fields, first we have "attrs varchar". This is the attribute string described earlier. For a given data point, it contains the serialized representation of all of the fields of that point aside from its value and timestamp. Note that attrs is also listed as the first field in the PRIMARY KEY. This makes attrs the "partition key" for our table, which means that each distinct attrs field defines a row. The second field and second component in the primary key is "offset int". The offset is the number of milliseconds past midnight on Thursday when a data point was recorded. There are fewer than 2^32 milliseconds in a week, so we can safely use an int for this field since we'll get a whole new table next Thursday. Since it is the second column in the primary key, Cassandra uses the offset as a "clustering column", meaning that points within a row are sorted according to their offset. This means that each of our rows in Cassandra is ordered by time, which is just what we want for our timeseries data model. The final field in our table is "value double". This is the value measured for a metric. 

So given a JSON object with a timestamp and a value, Orestes serializes the attributes other than timestamp and value into an attrs string, calculates what week the timestamp corresponds to and the offset within that week, and stores the attrs, offset and value the Cassandra table for the right week. The final words of our incantation are WITH COMPACT STORAGE. When storing a point using compact storage, Cassandra will not allocate any new space for its attrs string if that attrs string has been previously seen, instead storing just the time and value in the row corresponding to that attrs string. This means that Orestes can encode the points for a given metric in only 96 bytes apiece, plus the one-time cost of storing the serialized attrs string defining that metric.

Search

However, serializing the row keys in this way makes search very difficult. Let's say you wanted all the metrics for computer A. Since your data is not indexed by the computer field, only by the combination of the computer field and all other row-key fields, the only way to find all the metrics for computer A is to scan every row in Cassandra, parsing each row key into its component key-value pairs and returning the rows where the computer is A. Prior to the birth of Orestes, Jut used a timeseries database called KairosDB that implemented this approach. KairosDB was quick for the first thousand metrics or so, but it quickly slowed down due to these table scans as we added more metrics. At around 100,000 metrics, it became completely non-responsive. Even if we only queried a few metrics, Kairos would have to scan through all of them to find the ones we wanted, a completely broken model. We realized that in order to scale Jut to web-scale metric data, we would need a more efficient way of searching our row keys. 

We were already using Elasticsearch for event data, so it was natural to use it for our row keys as well. When Orestes imports a metric point, in addition to storing the serialized row key, time and value in Cassandra, it stores the JSON for the row key as a document in Elasticsearch. Elasticsearch maintains in memory an object called the "inverted index" that maps each key-value pair to the list of documents in which that key-value pair occurs. This enables Elasticsearch to very quickly answer queries such as our "get me all the row keys for computer A", and it opens up a whole world of pattern-matching, including free-text search, that would be impossible to do efficiently with Cassandra alone. Putting it all together, when a user runs a Juttle program asking a question about certain metrics, the Juttle compiler translates the Juttle filter for those metrics into an Elasticsearch filter matching the row keys for those metrics. It sends this Elasticsearch filter to Orestes, and Orestes executes a query to Elasticsearch to get the row keys matching that filter. Having retrieved the row keys, Orestes makes a select request to Cassandra for each row. Orestes sends the points retrieved from these requests to the Juttle Processing Core, which analyzes them to answer the user's question. 

Frequently, a user has questions about their metrics that don't involve the values for those metrics. For instance, you might want the list of all the metrics in your system, or you might wonder what the distinct names among your metrics are. Cassandra would have no quick way of answering these questions, but Elasticsearch can calculate them in a jiffy. For the first, Orestes has a method called read_tags. read_tags calls to Elasticsearch's scan and scroll API, which enables rapid paging through documents. read_tags can page through around 1,000,000 row keys per minute on a modern laptop using this API. For the second, Orestes has Elasticsearch perform a terms aggregation on the name field. This returns the list of distinct names in milliseconds, as Elasticsearch reads them from the inverted index. These are the APIs powering Juttle's streams command.

The Results

Garphs
