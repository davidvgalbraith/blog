Pushing the Limits of Node.js

The heart of the Jut platform is the Juttle Processing Core (JPC). The JPC is responsible for running Juttle programs. When you click play on a Juttle program, your browser sends the program to the JPC, which compiles the Juttle into Javascript. The JPC itself is written in Javascript, too, using node.js. We chose to use node.js for the JPC for several reasons. First, programming in a high-level language such as Javascript enables the rapid prototyping and iteration that a startup depends on. In addition, since the front end of our platform is written in Javascript, implementing the back end in Javascript as well makes it easy for developers to implement features end-to-end, without too many context switches or handoffs between front-end developers and back-end developers. Finally, node.js has a vibrant open-source community, so we can stand on the shoulders of giants such as moment.js, request.js, and bluebird.js. In fact, as of this writing, the JPC depends on 103 NPM packages, and Jut has open-sourced seven of our own, with more to come. 

So node.js offers a lot of attractive qualities when one is choosing a platform to build on. However, it also imposes a few restrictions, especially when your software has to deal with significant amounts of data, as the JPC does. First, a node.js application is single-threaded. This means that even if your computer has multiple CPU cores, as most computers do these days, your node.js application can only use one of them at any given time. Second, node.js can only use around 1.5 gigabytes of heap space. This limits the amount of space available to the JPC for storing the numbers, strings, and objects that it is performing computation on. If heap consumption approaches 1.5 GB, the application enters a state of near-constant garbage collection, which is the process of identifying objects that are no longer needed byt the program and freeing up their space so new objects can be allocated. Garbage collection is expensive, especially on a large heap, so this slows the process to a crawl. If you try to continue to allocate objects at this point, you will see the dreaded "FATAL ERROR: JS Allocation failed - process out of memory" message. That is not a happy message. It means your application has crashed. Here at Jut we have employed several tricks to achieve high performance on large data sets despite these limitations.

Perhaps the most powerful tool in Jut's big-data arsenal is Elasticsearch. We use Elasticsearch to store all event data. Elasticsearch has functionality called Aggregations, which perform computations across a set of data. The JPC has the ability to "optimize" Juttle programs that include the reduce processor, translating them into Elasticsearch Aggreagations. These optimized programs proceed much faster than processing events in Javascript to perform the computation. That is because an unoptimized approach would require Elasticsearch to pull all the relevant event data from disk, encode it as JSON, and send it over HTTP to the JPC, where the JPC would have to deserialize the JSON and perform the desired calculations. Getting rid of that overhead saves a lot of time. Furthermore, Elasticsearch is written in Java, so it can harness as many CPUs as are available when it needs to perform Aggregations. Thus, one effective way that we get high performance out of our node.js platform is to avoid doing computation in Javascript when possible. Unfortunately, the optimization approach does not work for many Juttle programs. This is because Juttle is much more expressive than the Elasticsearch Aggregation API. In particular, Elasticsearch Aggregations have no notion of merging or joining streams as Juttle does, and users are not allowed to write their own Aggregations. For these core features of Juttle and several others, we have to do all the computation in Javascript. 

The key to understanding node.js performance is the event loop. Basically, the event loop is a list of functions that node.js will invoke when certain events occur. When you tell your node.js server to make a request to another server, read a file from the filesystem, or do anything else that depends on an outside service, you also provide it with a function to call when that operation completes. node.js puts this function on its event loop, and when the outside operation completes, node.js applies the function you provided to the result of the outside operation. For instance, you can tell node.js to read some rows from a database (outside operation), then do some math on those rows when the database query completes (event loop function). This is essentially how the JPC works. Trouble occurs, however, when one of these event loop functions takes a long time to compute. Since node.js is single-threaded, it can only be actively processing one of its event loop functions at any point in time. So if the aforementioned database query returns a lot of rows, and the math you want to do on those rows is particularly involved, then node.js will spend a long time exclusively working on that. If other requests to your server are made during this time, or other outside operations requested by your server complete, they will just pile up on the event loop to-do list, waiting for the expensive query to complete. This will drive up the response time of your server, and if it falls too far behind it may never be able to catch up.

Therefore, avoiding situations where one function takes a long time to compute is the key to getting good performance out of a node.js server. In order to do this, we implement paging wherever possible. That means that when we need to read points from one of our data stores, we don't request them all at once. Instead, we fetch a few of them, then have node.js handle any other functions on its event loop before fetching the next batch. Of course, there are still trade-offs with this approach: each request has some overhead of its own, so if you make too many tiny requests, the program will still be slow, even though the event loop will never be blocked for an extended period of time. For Juttle, we have found that a fetch size of 20,000 points strikes a happy medium: node.js is able to perform the required computations for almost any Juttle program on 20,000 points in a few milliseconds, and it is still enough points that we can perform computations over millions of points without making too many requests.

A Case Study

One of Jut's beta customers is NPM, the company that makes the Node Package Manager. NPM is interested in finding the ten packages with the most downloads in the past two weeks, to fill out a table on their website. An easy Juttle program to compute this would be "read -last :2 weeks: | reduce count() by package | sort count -desc | head 10 | @table". Simple! Unfortunately, the first time they tried to run this program, it tied up the node.js CPU for over a minute. Jut has a process monitoring service that restarts the JPC if it does not respond to pings for a minute. This kicked in, the JPC was terminated, and NPM never got their data. I was called in to figure out what went wrong and to fix it. It turned out that the JPC had optimized the read/reduce combination here, making it into an Elasticsearch Terms Aggregation. Optimization backfired on us in this case, though, since the Terms Aggregation does not support any paging and NPM has close to a million packages. So Elasticsearch sent back a giant response with a million-item array containing all the results, with a total size of several hundred megabytes. The JPC attempted to process this all at once, and the additional overhead took us right up to the 1.5-gigabyte limit of node.js, so the JPC was stuck in garbage collection and never managed to get through all the data. 

To fix the program, I decided that even if Elasticsearch didn't give us paging for the aggregation, we could pretend it did. Instead of processing the whole giant result all at once, we could divide it into manageable chunks and process those one by one, yielding the CPU after each one. With the help of some open-source libraries, this was easy! The resulting Javascript code looks like this:

var points = perform_elasticsearch_aggregtion();

Promise.each(_.range(points.length / 20000), function processChunk(n) {
    return Promise.try(function() {
        process(points.splice(0, 20000));
    }).delay(1);
});

Promise.each is a handy utility added to the open-source bluebird.js library in 2014. Its arguments are an array and a function to perform on each item in the array. Promise.each traverses the array, calling the function on each item sequentially. If one of the function calls gives up the CPU before completion, Promise.each also gives up the CPU until that function resumes and completes. (This is the difference between Promise.each and the built-in Array.forEach, which will move on to the next item in the array it's traversing if one of its function calls yields the CPU). _.range is a simple function from the underscore.js library. _.range takes a number and returns an array of integers starting at 0 and ending one before that number. So for our million-item points array, _.range(points.length / 20000) returns the array [0, 1, 2, ... 49]. Using Promise.each, we apply the function processChunk to each of these numbers, for a total of 50 calls. Each call to processChunk pulls the first 20,000 points out of our array and calls "process" on them, which performs the computations needed for the Juttle program. This call to "process" is enclosed by Promise.try. Promise.try is just a wrapper from bluebird.js. It takes a function argument and returns an object with methods you can use to control the execution of that function. Here, we use the ".delay(1)" method, which yields the CPU for one millisecond after the function is done. Altogether, this gives us an implementation that processes our giant array in managable chunks of size 20,000, punctuated by brief pauses that enable the server to service other requests. After deploying this change, the NPM download-ranking program, which formerly locked up the JPC for over a minute, only took 20 seconds to complete, and the server was responsive to other requests for the whole duration. Cool!
