Welcome to your Data

Jut's vision is to bring all your data together in a single environment. This enables integrated analysis using the dataflow language Juttle. It is challenging because there are many different types of data. Different data types require different models for optimal storage and querying. At the highest level, Jut divides all data into two kingdoms: metrics and events.
Metrics

Metrics are one of the most useful tools in a devops agent's arsenal. Metric data is defined as repeated measurements of the same quantity as it varies over time. For instance, if I measure the CPU usage on my server every ten seconds while my application is running, that's metric data. Metric data is widely used to analyze the performance of software projects: there are tools like statsd and collectd that make it easy to collect. In a large system with many metrics, it becomes critical to store metric data in a compact way. The most popular data model for achieving this is the timeseries data model. A database using the timeseries data model is called a timeseries database. Given some data, a timeseries database figures out what metrics are being measured, and it associates each metric with a list of all the measurements of that metric. For instance, let's say I measured the CPU usage on computers A and B at 12:00, 1:00 and 2:00, and I got 10%, 20%, and 30% on computer A; 15%, 25%, and 35% on computer B. The timeseries representation of this data looks conceptually like:

        {metric_name: 'cpu_usage', computer: 'A'} ----> [{time: 12:00, value: 10%}, {time: 1:00, value: 20%}, {time: 2:00, value: 30%}]

        {metric_name: 'cpu_usage', computer: 'B'} ----> [{time: 12:00, value: 15%}, {time: 1:00, value: 25%}, {time: 2:00, value: 35%}]

A traditional relational database such as MySQL would store the metric name and computer with each time-value pair. Duplicating information like that takes up unnecessary space, so a timeseries database stores metric data more efficiently.
Events

Technically, Jut defines an event as "anything that is not a metric". Events that Jut frequently deals with are Github commits, segment.io analytics, and Fastly router actions. Application logs such as syslog and nginx also fall under the event umbrella. Jut's operations on events are powered by Elasticsearch. Elasticsearch is not a timeseries database. Events are not repeated measurements of the same quantity, so each point tends to be significantly different from the other points. If you try to apply the timeseries model to such data, you end up with a large number of groupings, each with few data points. This means the timeseries data model is not very useful for events.

Instead, Elasticsearch is a search engine. It stores JSON objects in data structures called documents. It maintains an object called the "inverted index" that maps each key-value pair to the list of documents in which that key-value pair occurs. By making lookups in the inverted index, Elasticsearch can quickly answer a wide range of questions about its stored data, including listing all the documents that contain a certain key-value pair or contain a string that the user is searching for. This supports Juttle's fast, expressive filter API.
Orestes

So we had a searchable event analytics platform built on Elasticsearch, but we wanted to take advantage of the timeseries model to efficiently store metrics as well. This cannot be achieved in Elasticsearch because its data model as a search engine requires too much metadata to be stored for every data point. This necessitated adding a new component to the Jut architecture to serve as a timeseries database. We needed our metrics to be as searchable as our events, so that Juttle's API could be independent of the type of data it is querying. It would be confusing to the user to have some filters that work only on events and others that work only on metrics. So our timeseries database needed to be as queryable as Elasticsearch. We investigated the open-source timeseries database solutions, including InfluxDB, KairosDB, OpenTSDB, Prometheus, and Gorilla, but none of them had APIs as expressive as Elasticsearch's. Therefore, we wrote our own timeseries database.  Jut's timeseries database is named Orestes. Orestes implements the timeseries data model on top of Apache Cassandra and uses Elasticsearch to build queries on its data. This combines the speed and scalability of Cassandra with the searchability of Elasticsearch to enable execution of flexible, high-performance queries on billions of data points across millions of metrics.
Cassandra

Orestes uses Apache Cassandra for most of its data storage and retrieval. This is because Cassandra's data model is easily adapted for timeseries data. Data in Cassandra is divided into "rows". A row is indexed by a "row key". Each row contains several "columns". A single stored object is represented by the combination of the key-value pairs of a column with the key-value pairs of the row containing it. Orestes represents a metric as a unique set of key-value pairs. This set of key-value pairs is used as a row key. The columns in each row are the times and values of measurement for the metric represented by that row's row key. So to represent the metric data above, one of our row keys would be {metric_name: 'cpu_usage', computer: 'A'}, corresponding to the columns [{time: 12:00, value: 10%}, {time: 1:00, value: 20%}, {time: 2:00, value: 30%}].

For storing a general JSON object representing a metric point, the row key consists of all the fields of the point other than time and value. Unfortunately, Cassandra requires users to fully specify the fields that make up the rows and columns of a table when that table is created. Therefore, unless you know in advance what all the attributes of all your data will be, there's no built-in way to tell Cassandra to use all the fields besides time and value as a row key. To get around this limitation, Orestes uses a single string field as the row key. The value of this field for a given data point is a serialized string representing the row-key fields of that point. For instance, Orestes would represent our row key {metric_name: 'cpu_usage', computer: 'A'} as "computer=A,metric_name=cpu_usage": an alphabetized, comma-delimited list of "key=value" susbtrings. This enables efficient storage of arbitrary timeseries data in Cassandra. Querying a row is quick, too. Given a row key, Cassandra can execute a read on that row in around ten microseconds on a modern laptop.

In addition to high-performance querying, we also wanted to have a configurable retention policy. Jut has a notion we call "space", which represents a logical division of data. For instance, data generated by tests might go in the "test" space, while data generated in production goes in the "production" space. We wanted users to be able to specify how many days of data to store in each space. Cassandra doesn't provide features that precisely correspond with these needs, but we were able to build a reasonable facsimile with the features it does have. In Cassandra, the highest level of organization is the "keyspace". Each keyspace can have several "tables". These tables contain the rows and columns that represent data points. Orestes makes use of all these features to support the functionality Jut requires.
An Orestes Table

A sample table-creation statement used by Orestes is "CREATE TABLE "orestes_default".metrics16688 (attrs varchar,offset int,value double, PRIMARY KEY(attrs, offset)) WITH COMPACT STORAGE". The table name encodes a fair bit of information: to the left of the dot is the name of the keyspace. Orestes keeps a keyspace for each Jut space that stores metric data, prefixing its name with "orestes_" to avoid collisions with Cassandra's built-in keyspaces. We enclose the name of the keyspace in quotes to allow spaces with special characters.

On the right side of the dot, we have the name of a particular table. It is just the word "metrics" with an integer suffix. The suffix represents the number of days since January 1, 1970 at the time of that table's creation. The suffix will always be a multiple of 7, because Orestes creates one table per week per keyspace. We went with this model to enable quick deletion of unwanted metric data. To delete unwanted data, Orestes just drops the table containing that data. This means that if you want to delete any data from Orestes, you need to delete the whole week that contains that data. This is an unfortunate restriction, but it was the best way we found of working with Cassandra's deletion quirkiness. Cassandra ostensibly has a delete-by-query API, but this API is a lie. When you delete data using Cassandra's delete-by-query API, it really just places "tombstones" on the datapoints you deleted, to be cleaned up the next time Cassandra performs a compaction on the table with tombstones. Compaction is part of Cassandra's self-regulated maintenance, but it occurs at unpredictable intervals and very infrequently if a table isn't being written to. Tombstoned points take up as much disk space as non-tombstoned points, so keeping them around for an arbitrary period of time after you've allegedly deleted them was not an option for us. Luckily, Cassandra deletes data and frees disk space immediately upon dropping a table, provided you have "auto_snapshot: false" set in your cassandra.yaml configuration file (with auto_snapshot: true, the default, Cassandra will back up all the data you just tried to delete, and space still won't be freed). So that's how we got Cassandra to delete data. We chose a granularity of one week instead of something smaller because Cassandra has scaling problems with many tables: each table requires around a megabyte of heap overhead for pre-allocated buffers and such, so if you have thousands of tables you'll run into memory pressure. With only one table per week, we can support scores of spaces.

Moving on to the table fields, first we have "attrs varchar". This is the row key string described earlier. For a given data point, it contains the serialized representation of all of the fields of that point aside from its value and timestamp. A unique attrs string defines a metric to Orestes. Note that attrs is also listed as the first field in the PRIMARY KEY. This makes attrs the "partition key" for our table, which means that each distinct attrs field defines a row. The second field and second component in the primary key is "offset int". The offset is the number of milliseconds past midnight on Thursday when a data point was recorded. There are fewer than 2^32 milliseconds in a week, so we can use an int for this field since we'll get a whole new table next Thursday. Since it is the second column in the primary key, Cassandra uses the offset as a "clustering column", meaning that points within a row are sorted according to their offset. This means that each of our rows in Cassandra is ordered by time, which is just what we want for a timeseries data model. The final field in our table is "value double". This is the value measured for a metric.

So given a JSON object with a timestamp and a value for insertion, Orestes serializes the attributes other than timestamp and value into an attrs string, calculates what week the timestamp corresponds to and the offset within that week, and stores the attrs, offset and value the Cassandra table for the right week. The final words of our incantation are WITH COMPACT STORAGE. When storing a point using compact storage, Cassandra will not allocate any new space for its attrs string if it has already seen that attrs string, instead storing just the time and value in the row corresponding to that attrs string. This means that Orestes can encode the points for a given metric in 96 bytes apiece (64 for the double value and 32 for the int offset), plus the one-time cost of storing the serialized attrs string defining that metric.
Search

However, serializing the row keys in this way makes search difficult. Let's say you wanted all the metrics for computer A. But the metrics in Cassandra are not indexed by the computer field, only by the combination of the computer field and all other row-key fields. The only way to find all the metrics for computer A is to scan every row in Cassandra, parsing each row key into its component key-value pairs and returning the rows where the computer is A. This approach is workable for a few thousand metrics, but Jut's systems require storage of millions of metrics. With millions of metrics, a table scan like this cripples Cassandra on every search. In order to make Jut work on web-scale metric data, we needed a more efficient way of searching our row keys. This is where Elasticsearch enters the picture.

When Orestes imports a metric point, in addition to storing the serialized row key, time and value in Cassandra, it stores a JSON document in Elasticsearch with the key-value pairs defining the row key. The _id field of this document is set to the serialized row key string. This means that Orestes will only store one document in Elasticsearch for each metric it has stored in Cassandra. This duplicates the metric metadata once (the information is now in Elasticsearch as well as Cassandra), but it is still a many-thousandfold improvement over storing it for every data point. Furthermore, it gives Orestes access to the whole Elasticsearch API for searching its stored metrics. When a user runs a Juttle program asking a question about certain metrics, the Juttle compiler translates the Juttle filter into an Elasticsearch filter matching the row keys for those metrics. The Juttle runtime sends this Elasticsearch filter to Orestes, and Orestes executes a query to Elasticsearch to get the row keys matching that filter. Having retrieved the row keys, Orestes makes a select request to Cassandra for each row. Orestes sends the points retrieved from these requests to the Juttle Processing Core, which analyzes them to answer the user's question. This gives Juttle exactly as much expressiveness in its metric filters as it has in its event filters.

Additionally, users frequently have questions about their metrics that don't involve the values for those metrics. For instance, you might want the list of all the metrics in your system, or you might wonder what the distinct names among your metrics are. Cassandra would have no quick way of answering these questions, but Elasticsearch can calculate them in a jiffy. For the first, Orestes calls to Elasticsearch's scan and scroll API, which enables rapid paging through documents. Scan and scroll can page through around 1,000,000 row keys per minute on a modern laptop. For the second, Orestes has Elasticsearch perform a terms aggregation on the name field. This returns the list of distinct names in a matter of milliseconds, as Elasticsearch reads them from the inverted index. These are the APIs powering Juttle's streams command.
The Results

// XXX get statistics on Orestes' scale in production
